k means clustering Wikipedia k means clustering From Wikipedia free encyclopedia Jump navigation Jump search Vector quantization algorithm minimizing sum squared deviations Machine learning data mining Problems Classification Clustering Regression Anomaly detection AutoML Association rules Reinforcement learning Structured prediction Feature engineering Feature learning Online learning Semi supervised learning Unsupervised learning Learning rank Grammar induction Supervised learning classification regression Decision trees Ensembles Bagging Boosting Random forest k NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine RVM Support vector machine SVM Clustering BIRCH CURE Hierarchical k means Expectation maximization EM DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA t SNE Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k NN Local outlier factor Artificial neural networks Autoencoder Deep learning DeepDream Multilayer perceptron RNN LSTM GRU Restricted Boltzmann machine GAN SOM Convolutional neural network U Net Reinforcement learning Q learning SARSA Temporal difference TD Theory Bias variance dilemma Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machine learning venues NeurIPS ICML ML JMLR ArXiv cs LG Glossary artificial intelligence Glossary artificial intelligence Related articles List datasets machine learning research Outline machine learning v t e k means clustering method vector quantization originally from signal processing that popular cluster analysis in data mining k means clustering aims partition n observations into k clusters in which each observation belongs cluster with nearest mean serving as prototype cluster This results in partitioning data space into Voronoi cells k Means minimizes within cluster variances squared Euclidean distances but not regular Euclidean distances which would be more difficult Weber problem mean optimizes squared errors whereas only geometric median minimizes Euclidean distances Better Euclidean solutions can example be found using k medians k medoids problem computationally difficult NP hard however efficient heuristic algorithms converge quickly local optimum These are usually similar expectation maximization algorithm mixtures Gaussian distributions via an iterative refinement approach employed by both k means Gaussian mixture modeling They both use cluster centers model data however k means clustering tends find clusters comparable spatial extent while expectation maximization mechanism allows clusters have different shapes algorithm has loose relationship k nearest neighbor classifier popular machine learning technique classification that often confused with k means due name Applying nearest neighbor classifier cluster centers obtained by k means classifies new data into existing clusters This known as nearest centroid classifier Rocchio algorithm Contents Description History Algorithms Standard algorithm naive k means Initialization methods Complexity Variations Hartigan Wong method Discussion Applications Vector quantization Cluster analysis Feature learning Relation other algorithms Gaussian mixture model Principal component analysis Mean shift clustering Independent component analysis Bilateral filtering Similar problems Software implementations Free Software Open Source Proprietary See also References Description edit Given set observations x x xn each observation d dimensional real vector k means clustering aims partition n observations into k n sets S S S Sk so as minimize within cluster sum squares WCSS i e variance Formally objective find r g m i n S i k x S i x i r g m i n S i k S i Var S i displaystyle underset mathbf S operatorname arg min sum i k sum mathbf x in S i left mathbf x boldsymbol mu i right underset mathbf S operatorname arg min sum i k S i operatorname Var S i i mean points in Si This equivalent minimizing pairwise squared deviations points in same cluster r g m i n S i k S i x y S i x y displaystyle underset mathbf S operatorname arg min sum i k frac S i sum mathbf x mathbf y in S i left mathbf x mathbf y right equivalence can be deduced from identity x S i x i x y S i x i i y displaystyle sum mathbf x in S i left mathbf x boldsymbol mu i right sum mathbf x neq mathbf y in S i mathbf x boldsymbol mu i boldsymbol mu i mathbf y total variance constant this equivalent maximizing sum squared deviations between points in different clusters between cluster sum squares BCSS which follows from law total variance History edit term k means was first used by James MacQueen in though idea goes back Hugo Steinhaus in standard algorithm was first proposed by Stuart Lloyd Bell Labs in as technique pulse code modulation though it wasn t published as journal article until In Edward W Forgy published essentially same method which it sometimes referred as Lloyd Forgy Algorithms edit Standard algorithm naive k means edit Convergence k means most common algorithm uses an iterative refinement technique Due its ubiquity it often called k means algorithm it also referred as Lloyd s algorithm particularly in computer science community It sometimes also referred as naive k means exist much faster alternatives Given an initial set k means m mk see below algorithm proceeds by alternating between two steps Assignment step Assign each observation cluster whose mean has least squared Euclidean distance this intuitively nearest mean Mathematically this means partitioning observations according Voronoi diagram generated by means S i t x p x p m i t x p m j t j j k displaystyle S i t big x p big x p m i t big leq big x p m j t big forall j leq j leq k big each x p displaystyle x p assigned exactly one S t displaystyle S t even if it could be assigned two more them Update step Calculate new means centroids observations in new clusters m i t S i t x j S i t x j displaystyle m i t frac left S i t right sum x j in S i t x j algorithm has converged assignments no longer change algorithm does not guarantee find optimum algorithm often presented as assigning objects nearest cluster by distance Using different distance function other than squared Euclidean distance may stop algorithm from converging Various modifications k means such as spherical k means k medoids have been proposed allow using other distance measures Initialization methods edit Commonly used initialization methods are Forgy Random Partition Forgy method randomly chooses k observations from dataset uses these as initial means Random Partition method first randomly assigns cluster each observation then proceeds update step thus computing initial mean be centroid cluster s randomly assigned points Forgy method tends spread initial means out while Random Partition places all them close center data set According Hamerly et al Random Partition method generally preferable algorithms such as k harmonic means fuzzy k means expectation maximization standard k means algorithms Forgy method initialization preferable comprehensive study by Celebi et al however found that popular initialization methods such as Forgy Random Partition Maximin often perform poorly whereas Bradley Fayyad s approach performs consistently in best group k means performs generally well Demonstration standard algorithm k initial means in this case k are randomly generated within data domain shown in color k clusters are created by associating every observation with nearest mean partitions here represent Voronoi diagram generated by means centroid each k clusters becomes new mean Steps are repeated until convergence has been reached algorithm does not guarantee convergence global optimum result may depend on initial clusters As algorithm usually fast it common run it multiple times with different starting conditions However worst case performance can be slow in particular certain point sets even in two dimensions converge in exponential time that n These point sets do not seem arise in practice this corroborated by fact that smoothed running time k means polynomial assignment step referred as expectation step while update step maximization step making this algorithm variant generalized expectation maximization algorithm Complexity edit Finding optimal solution k means clustering problem observations in d dimensions NP hard in general Euclidean space d dimensions even two clusters NP hard general number clusters k even in plane if k d dimension are fixed problem can be exactly solved in time O n d k displaystyle O n dk n number entities be clustered Thus variety heuristic algorithms such as Lloyd s algorithm given above are generally used running time Lloyd s algorithm most variants O n k d i displaystyle O nkdi n number d dimensional vectors be clustered k number clusters i number iterations needed until convergence On data that does have clustering structure number iterations until convergence often small results only improve slightly after first dozen iterations Lloyd s algorithm therefore often considered be linear complexity in practice although it in worst case superpolynomial performed until convergence In worst case Lloyd s algorithm needs i n displaystyle i Omega sqrt n iterations so that worst case complexity Lloyd s algorithm superpolynomial Lloyd s k means algorithm has polynomial smoothed running time It shown that arbitrary set n points in d displaystyle d if each point independently perturbed by normal distribution with mean variance displaystyle sigma then expected running time k means algorithm bounded by O n k d log n displaystyle O n k d log n sigma which polynomial in n k d displaystyle sigma Better bounds are proven simple cases example in it shown that running time k means algorithm bounded by O d n M displaystyle O dn M n points in an integer lattice M d displaystyle dots M d Lloyd s algorithm standard approach this problem However it spends lot processing time computing distances between each k cluster centers n data points Since points usually stay in same clusters after few iterations much this work unnecessary making naive implementation very inefficient Some implementations use caching triangle inequality in order create bounds accelerate Lloyd s algorithm Variations edit Jenks natural breaks optimization k means applied univariate data k medians clustering uses median in each dimension instead mean this way minimizes L displaystyle L norm Taxicab geometry k medoids also Partitioning Around Medoids PAM uses medoid instead mean this way minimizes sum distances arbitrary distance functions Fuzzy C Means Clustering soft version k means each data point has fuzzy degree belonging each cluster Gaussian mixture models trained with expectation maximization algorithm EM algorithm maintains probabilistic assignments clusters instead deterministic assignments multivariate Gaussian distributions instead means k means chooses initial centers in way that gives provable upper bound on WCSS objective filtering algorithm uses kd trees speed up each k means step Some methods attempt speed up each k means step using triangle inequality Escape local optima by swapping points between clusters Spherical k means clustering algorithm suitable textual data Hierarchical variants such as Bisecting k means X means clustering G means clustering repeatedly split clusters build hierarchy can also try automatically determine optimal number clusters in dataset Internal cluster evaluation measures such as cluster silhouette can be helpful at determining number clusters Minkowski weighted k means automatically calculates cluster specific feature weights supporting intuitive idea that feature may have different degrees relevance at different features These weights can also be used re scale given data set increasing likelihood cluster validity index be optimized at expected number clusters Mini batch k means k means variation using mini batch samples data sets that do not fit into memory Hartigan Wong method edit Hartigan Wong s method provides more sophisticated though more computationally expensive way perform k means It still heuristic method S j displaystyle varphi S j individual cost S j displaystyle S j defined by x S j x j displaystyle sum x in S j x mu j with j displaystyle mu j center cluster Assignment step Hartigan Wong s method starts by partitioning points into random clusters S j j k displaystyle S j j in cdots k Update step Next it determines n m k displaystyle n m in ldots k x S n displaystyle x in S n which following function reaches minimum m n x S n S m S n x S m x displaystyle Delta m n x varphi S n varphi S m varphi S n smallsetminus x varphi S m cup x x n m displaystyle x n m that reach this minimum x displaystyle x moves from cluster S n displaystyle S n cluster S m displaystyle S m Termination algorithm terminates once m n x displaystyle Delta m n x larger than zero all x n m displaystyle x n m algorithm can be sped up by immediately moving x displaystyle x from cluster S n displaystyle S n cluster S m displaystyle S m as soon as an x n m displaystyle x n m have been found which m n x displaystyle Delta m n x This speed up can make cost final result higher function displaystyle Delta can be relatively efficiently evaluated by making use equality x n m S n S n n x S m S m m x displaystyle Delta x n m frac mid S n mid mid S n mid cdot lVert mu n x rVert frac mid S m mid mid S m mid cdot lVert mu m x rVert Discussion edit typical example k means convergence local minimum In this example result k means clustering right figure contradicts obvious cluster structure data set small circles are data points four ray stars are centroids means initial configuration on left figure algorithm converges after five iterations presented on figures from left right illustration was prepared with Mirkes Java applet k means clustering result Iris flower data set actual species visualized using ELKI Cluster means are marked using larger semi transparent symbols k means clustering vs EM clustering on an artificial dataset mouse tendency k means produce equal sized clusters leads bad results here while EM benefits from Gaussian distributions with different radius present in data set Three key features k means that make it efficient are often regarded as its biggest drawbacks Euclidean distance used as metric variance used as measure cluster scatter number clusters k an input parameter an inappropriate choice k may yield poor results That performing k means it important run diagnostic checks determining number clusters in data set Convergence local minimum may produce counterintuitive wrong results see example in Fig key limitation k means its cluster model concept based on spherical clusters that are separable so that mean converges towards cluster center clusters are expected be similar size so that assignment nearest cluster center correct assignment example applying k means with value k displaystyle k onto well known Iris flower data set result often fails separate three Iris species contained in data set With k displaystyle k two visible clusters one containing two species will be discovered whereas with k displaystyle k one two clusters will be split into two even parts In fact k displaystyle k more appropriate this data set despite data set s containing classes As with any other clustering algorithm k means result makes assumptions that data satisfy certain criteria It works well on some data sets fails on others result k means can be seen as Voronoi cells cluster means Since data split halfway between cluster means this can lead suboptimal splits as can be seen in mouse example Gaussian models used by expectation maximization algorithm arguably generalization k means are more flexible by having both variances covariances EM result thus able accommodate clusters variable size much better than k means as well as correlated clusters not in this example K means closely related nonparametric Bayesian modeling Applications edit k means clustering rather easy apply even large data sets particularly using heuristics such as Lloyd s algorithm It has been successfully used in market segmentation computer vision astronomy among many other domains It often used as preprocessing step other algorithms example find starting configuration Vector quantization edit Main article Vector quantization Two channel illustration purposes red green only color image Vector quantization colors present in image above into Voronoi cells using k means k means originates from signal processing still finds use in this domain example in computer graphics color quantization task reducing color palette an image fixed number colors k k means algorithm can easily be used this task produces competitive results use case this approach image segmentation Other uses vector quantization include non random sampling as k means can easily be used choose k different but prototypical objects from large data set further analysis Cluster analysis edit Main article Cluster analysis In cluster analysis k means algorithm can be used partition input data set into k partitions clusters However pure k means algorithm not very flexible as such limited use except vector quantization as above actually desired use case In particular parameter k known be hard choose as discussed above not given by external constraints Another limitation that it cannot be used with arbitrary distance functions on non numerical data these use cases many other algorithms are superior Feature learning edit k means clustering has been used as feature learning dictionary learning step in either semi supervised learning unsupervised learning basic approach first train k means clustering representation using input training data which need not be labelled Then project any input datum into new feature space an encoding function such as thresholded matrix product datum with centroid locations computes distance from datum each centroid simply an indicator function nearest centroid some smooth transformation distance Alternatively transforming sample cluster distance through Gaussian RBF obtains hidden layer radial basis function network This use k means has been successfully combined with simple linear classifiers semi supervised learning in NLP specifically named entity recognition in computer vision On an object recognition task it was found exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders restricted Boltzmann machines However it generally requires more data equivalent performance each data point only contributes one feature Relation other algorithms edit Gaussian mixture model edit k means clustering its associated expectation maximization algorithm special case Gaussian mixture model specifically limit taking all covariances as diagonal equal small It often easy generalize k means problem into Gaussian mixture model Another generalization k means algorithm K SVD algorithm which estimates data points as sparse linear combination codebook vectors k means corresponds special case using single codebook vector with weight Principal component analysis edit relaxed solution k means clustering specified by cluster indicators given by principal component analysis PCA PCA subspace spanned by principal directions identical cluster centroid subspace intuition that k means describe spherically shaped ball like clusters If data has clusters line connecting two centroids best dimensional projection direction which also first PCA direction Cutting line at center mass separates clusters this continuous relaxation discrete cluster indicator If data have three clusters dimensional plane spanned by three cluster centroids best D projection This plane also defined by first two PCA dimensions Well separated clusters are effectively modeled by ball shaped clusters thus discovered by k means Non ball shaped clusters are hard separate they are close example two half moon shaped clusters intertwined in space do not separate well projected onto PCA subspace k means should not be expected do well on this data It straightforward produce counterexamples statement that cluster centroid subspace spanned by principal directions Mean shift clustering edit Basic mean shift clustering algorithms maintain set data points same size as input data set Initially this set copied from input set Then this set iteratively replaced by mean those points in set that are within given distance that point By contrast k means restricts this updated set k points usually much less than number points in input data set replaces each point in this set by mean all points in input set that are closer that point than any other e g within Voronoi partition each updating point mean shift algorithm that similar then k means called likelihood mean shift replaces set points undergoing replacement by mean all points in input set that are within given distance changing set One advantages mean shift over k means that number clusters not pre specified mean shift likely find only few clusters if only small number exist However mean shift can be much slower than k means still requires selection bandwidth parameter Mean shift has soft variants Independent component analysis edit Under sparsity assumptions input data pre processed with whitening transformation k means produces solution linear independent component analysis ICA task This aids in explaining successful application k means feature learning Bilateral filtering edit k means implicitly assumes that ordering input data set does not matter bilateral filter similar k means mean shift in that it maintains set data points that are iteratively replaced by means However bilateral filter restricts calculation kernel weighted mean include only points that are close in ordering input data This makes it applicable problems such as image denoising spatial arrangement pixels in an image critical importance Similar problems edit set squared error minimizing cluster functions also includes k medoids algorithm an approach which forces center point each cluster be one actual points i e it uses medoids in place centroids Software implementations edit Different implementations algorithm exhibit performance differences with fastest on test data set finishing in seconds slowest taking seconds hours differences can be attributed implementation quality language compiler differences different termination criteria precision levels use indexes acceleration Free Software Open Source edit following implementations are available under Free Open Source Software licenses with publicly available source code Accord NET contains C implementations k means k means k modes ALGLIB contains parallelized C C implementations k means k means AOSP contains Java implementation k means CrimeStat implements two spatial k means algorithms one which allows user define starting locations ELKI contains k means with Lloyd MacQueen iteration along with different initializations such as k means initialization various more advanced clustering algorithms Julia contains k means implementation in JuliaStats Clustering package KNIME contains nodes k means k medoids Mahout contains MapReduce based k means mlpack contains C implementation k means Octave contains k means OpenCV contains k means implementation Orange includes component k means clustering with automatic selection k cluster silhouette scoring PSPP contains k means QUICK CLUSTER command performs k means clustering on dataset R contains three k means variations SciPy scikit learn contain multiple k means implementations Spark MLlib implements distributed k means algorithm Torch contains an unsup package that provides k means clustering Weka contains k means x means Proprietary edit following implementations are available under proprietary license terms may not have publicly available source code Ayasdi Mathematica MATLAB OriginPro RapidMiner SAP HANA SAS SPSS Stata See also edit BFR algorithm Centroidal Voronoi tessellation Head tail Breaks k q flats K means Linde Buzo Gray algorithm Self organizing map References edit b Kriegel Hans Peter Schubert Erich Zimek Arthur black art runtime evaluation Are we comparing algorithms implementations Knowledge Information Systems doi s ISSN MacQueen J B Some Methods classification Analysis Multivariate Observations Proceedings th Berkeley Symposium on Mathematical Statistics Probability University California Press pp MR Zbl Retrieved Steinhaus Hugo Sur la division des corps mat riels en parties Bull Acad Polon Sci in French MR Zbl Lloyd Stuart P Least square quantization in PCM Bell Telephone Laboratories Paper Published in journal much later Lloyd Stuart P Least squares quantization in PCM PDF IEEE Transactions on Information Theory CiteSeerX doi TIT Retrieved Forgy Edward W Cluster analysis multivariate data efficiency versus interpretability classifications Biometrics JSTOR Pelleg Dan Moore Andrew Accelerating exact k means algorithms with geometric reasoning Proceedings fifth ACM SIGKDD international conference on Knowledge discovery data mining KDD San Diego California United States ACM Press doi ISBN MacKay David Chapter An Example Inference Task Clustering PDF Information Theory Inference Learning Algorithms Cambridge University Press pp ISBN MR Since square root monotone function this also minimum Euclidean distance assignment b c d e Hartigan J Wong M Algorithm AS k Means Clustering Algorithm Journal Royal Statistical Society Series C JSTOR b Hamerly Greg Elkan Charles Alternatives k means algorithm that find better clusterings PDF Proceedings eleventh international conference on Information knowledge management CIKM Celebi M E Kingravi H Vela P comparative study efficient initialization methods k means clustering algorithm Expert Systems with Applications arXiv doi j eswa Bradley Paul S Fayyad Usama M Refining Initial Points k Means Clustering Proceedings Fifteenth International Conference on Machine Learning Vattani k means requires exponentially many iterations even in plane PDF Discrete Computational Geometry doi s b Arthur David Manthey B Roeglin H k means has polynomial smoothed complexity Proceedings th Symposium on Foundations Computer Science FOCS arXiv Garey M Johnson D Witsenhausen H complexity generalized Lloyd Max problem Corresp IEEE Transactions on Information Theory doi TIT ISSN Kleinberg Jon Papadimitriou Christos Raghavan Prabhakar Microeconomic View Data Mining Data Mining Knowledge Discovery doi ISSN Aloise D Deshpande Hansen P Popat P NP hardness Euclidean sum squares clustering Machine Learning doi s Dasgupta S Freund Y July Random Projection Trees Vector Quantization IEEE Transactions on Information Theory arXiv doi TIT Mahajan Meena Nimbhorkar Prajakta Varadarajan Kasturi Planar k Means Problem NP Hard Lecture Notes in Computer Science pp CiteSeerX doi ISBN Inaba M Katoh N Imai H Applications weighted Voronoi diagrams randomization variance based k clustering Proceedings th ACM Symposium on Computational Geometry pp doi Manning Christopher D Raghavan Prabhakar Sch tze Hinrich Introduction information retrieval New York Cambridge University Press ISBN OCLC b Arthur David Vassilvitskii Sergei Slow k means Method Proceedings Twenty second Annual Symposium on Computational Geometry SCG New York NY USA ACM pp doi ISBN Bhowmick Abhishek theoretical analysis Lloyd s algorithm k means clustering PDF Thesis See also here b Phillips Steven J Acceleration K Means Related Clustering Algorithms In Mount David M Stein Clifford eds Acceleration k Means Related Clustering Algorithms Lecture Notes in Computer Science Springer Berlin Heidelberg pp doi ISBN b Elkan Charles Using triangle inequality accelerate k means PDF Proceedings Twentieth International Conference on Machine Learning ICML b Hamerly Greg Making k means even faster CiteSeerX Cite journal requires journal help b Hamerly Greg Drake Jonathan Accelerating Lloyd s algorithm k means clustering Partitional Clustering Algorithms pp doi ISBN Kanungo Tapas Mount David M Netanyahu Nathan S Piatko Christine D Silverman Ruth Wu Angela Y An efficient k means clustering algorithm Analysis implementation PDF IEEE Transactions on Pattern Analysis Machine Intelligence doi TPAMI Retrieved Drake Jonathan Accelerated k means with adaptive distance bounds PDF th NIPS Workshop on Optimization Machine Learning OPT Dhillon S Modha D M Concept decompositions large sparse text data using clustering Machine Learning doi Steinbach M Karypis G Kumar V August comparison document clustering techniques In KDD workshop on text mining Vol No pp Pelleg D Moore W June X means Extending k means with Efficient Estimation Number Clusters In ICML Vol Hamerly Greg Elkan Charles Learning k in k means In Advances in neural information processing systems Amorim R C Mirkin B Minkowski Metric Feature Weighting Anomalous Cluster Initialisation in k Means Clustering Pattern Recognition doi j patcog Amorim R C Hennig C Recovering number clusters in data sets with noise features using feature rescaling factors Information Sciences arXiv doi j ins Sculley David Web scale k means clustering Proceedings th international conference on World Wide Web ACM pp Retrieved Telgarsky Matus Hartigan s Method k means Clustering without Voronoi PDF Mirkes E M K means k medoids applet Retrieved January Kulis Brian Jordan Michael Revisiting k means new algorithms via Bayesian nonparametrics PDF ICML pp ISBN b c Coates Adam Ng Andrew Y Learning feature representations with k means PDF In Montavon G Orr G B M ller K R eds Neural Networks Tricks Trade Springer CS maint uses editors parameter link Csurka Gabriella Dance Christopher C Fan Lixin Willamowski Jutta Bray C dric Visual categorization with bags keypoints PDF ECCV Workshop on Statistical Learning in Computer Vision b Coates Adam Lee Honglak Ng Andrew Y An analysis single layer networks in unsupervised feature learning PDF International Conference on Artificial Intelligence Statistics AISTATS Archived from original PDF on Schwenker Friedhelm Kestler Hans Palm G nther Three learning phases radial basis function networks Neural Networks CiteSeerX doi s Lin Dekang Wu Xiaoyun Phrase clustering discriminative learning PDF Annual Meeting ACL IJCNLP pp Press W H Teukolsky S Vetterling W T Flannery B P Section Gaussian Mixture Models k Means Clustering Numerical Recipes Art Scientific Computing rd ed New York NY Cambridge University Press ISBN Aharon Michal Elad Michael Bruckstein Alfred K SVD An Algorithm Designing Overcomplete Dictionaries Sparse Representation PDF IEEE Transactions on Signal Processing Bibcode ITSP doi TSP Archived from original PDF on Zha Hongyuan Ding Chris Gu Ming He Xiaofeng Simon Horst D December Spectral Relaxation k means Clustering PDF Neural Information Processing Systems Vol NIPS Ding Chris He Xiaofeng July K means Clustering via Principal Component Analysis PDF Proceedings International Conference on Machine Learning ICML Drineas Petros Frieze Alan M Kannan Ravi Vempala Santosh Vinay Vishwanathan Clustering large graphs via singular value decomposition PDF Machine Learning doi b mach Retrieved Cohen Michael B Elder Sam Musco Cameron Musco Christopher Persu Madalina Dimensionality reduction k means clustering low rank approximation Appendix B arXiv cs DS b Little Max Jones Nick S Generalized Methods Solvers Piecewise Constant Signals Part PDF Proceedings Royal Society Bibcode RSPSA L doi rspa PMC PMID Vinnikov Alon Shalev Shwartz Shai K means Recovers ICA Filters Independent Components are Sparse PDF Proceedings International Conference on Machine Learning ICML Retrieved from https en wikipedia org w index php title K means clustering oldid Categories Cluster analysis algorithms Hidden categories CS French language sources fr CS errors missing periodical CS maint uses editors parameter Articles with short description Navigation menu Personal tools Not logged in Talk Contributions Create account Log in Namespaces Article Talk Variants Views Read Edit View history More Search Navigation Main page Contents Featured content Current events Random article Donate Wikipedia Wikipedia store Interaction Help About Wikipedia Community portal Recent changes Contact page Tools links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page Print export Create book Download as PDF Printable version Languages Catal e tina Deutsch Eesti Espa ol Fran ais Bahasa Indonesia Italiano Norsk Polski Portugu s srpski T rk e Edit links This page was last edited on November at UTC Text available under Creative Commons Attribution ShareAlike License additional terms may apply By using this site agree Terms Use Privacy Policy Wikipedia registered trademark Wikimedia Foundation Inc non profit organization Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Statistics Cookie statement Mobile view